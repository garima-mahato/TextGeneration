{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Text_Generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7B_6nNhYab9",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation using LSTM\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P47ZpqpwYgTV",
        "colab_type": "text"
      },
      "source": [
        "### Initial Setup\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g6h1G9fypOf",
        "colab_type": "code",
        "outputId": "11dfe68c-b1a8-4686-c6f9-8598a0c79124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131331 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.6-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.6-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.6-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib9QRNEJzS8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xJ0kuLc0aqn",
        "colab_type": "code",
        "outputId": "85e701f6-b7ad-497a-aba3-fad361a707b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJLqFWtnzXzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeTFKXNEzYVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "default_path = 'drive/LSTM_TextGen/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9Gi7d-91RLY",
        "colab_type": "text"
      },
      "source": [
        "### LSTM Model Creation\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvJP6tNDRnyZ",
        "colab_type": "code",
        "outputId": "2e094c39-19d4-4648-d98f-fb43533162e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import string\n",
        "import math\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n",
        "orig_text = open(default_path + filename).read()\n",
        "orig_text = orig_text.lower()\n",
        "\n",
        "# remove punctuations\n",
        "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "raw_text = regex.sub('', orig_text)\n",
        "\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))[1:]\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  136087\n",
            "Total Vocab:  29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gUzapAGdqMT",
        "colab_type": "code",
        "outputId": "373cf71c-0f96-4265-9c20-583be4da5775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "lines = [line for line in raw_text.splitlines() if line.isspace()==False and len(line)>0]\n",
        "#print(lines)\n",
        "\n",
        "sent_len_list = [len(line) for line in lines]\n",
        "max_sent_len = max(sent_len_list)\n",
        "min_sent_len = min(sent_len_list)\n",
        "avg_sent_len = math.floor(sum(sent_len_list)/len(sent_len_list))\n",
        "common_sent_len = max(set(sent_len_list), key=sent_len_list.count)\n",
        "print(f\"Maximum length of sentence - {max_sent_len}\")\n",
        "print(f\"Minimum length of sentence - {min_sent_len}\")\n",
        "print(f\"Average length of sentences - {avg_sent_len}\")\n",
        "print(f\"Common length of sentences - {common_sent_len}\")\n",
        "\n",
        "inp_sequences = [list(line) for line in lines]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum length of sentence - 73\n",
            "Minimum length of sentence - 2\n",
            "Average length of sentences - 53\n",
            "Common length of sentences - 67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3loQ5aS9dnzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "data = []\n",
        "for line in lines:\n",
        "\tdata.append([char_to_int[char] for char in line])\n",
        "#n_patterns = len(data)\n",
        "#print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn7YmiVS_xis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# pad sequence\n",
        "data_padded = pad_sequences(data, maxlen=max_sent_len)\n",
        "flat_list = [item for sublist in data_padded for item in sublist]\n",
        "\n",
        "input_len = len(flat_list)\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsJ_mgdaePtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, input_len - seq_length, 1):\n",
        "    # Define input and output sequences\n",
        "    # Input is the current character plus desired sequence length\n",
        "    in_seq = flat_list[i:i + seq_length]\n",
        "\n",
        "    # Out sequence is the initial character plus total sequence length\n",
        "    out_seq = flat_list[i + seq_length]\n",
        "\n",
        "    # We now convert list of characters to integers based on\n",
        "    # previously and add the values to our lists\n",
        "    dataX.append([char for char in in_seq])\n",
        "    dataY.append(out_seq)\n",
        "\n",
        "n_patterns = len(dataX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv4keJlMJGvA",
        "colab_type": "code",
        "outputId": "b9321b54-450e-4530-99f5-ae37c3b8c9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# define the checkpoint\n",
        "filepath = default_path + \"Models/Model5/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=512, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0727 10:13:32.494488 140176973526912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0727 10:13:32.522053 140176973526912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0727 10:13:32.529314 140176973526912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0727 10:13:32.807087 140176973526912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0727 10:13:32.818583 140176973526912 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0727 10:13:33.938630 140176973526912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0727 10:13:33.967402 140176973526912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0727 10:13:34.114025 140176973526912 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "180283/180283 [==============================] - 207s 1ms/step - loss: 2.3143\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.31433, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-01-2.3143-bigger.hdf5\n",
            "Epoch 2/100\n",
            "180283/180283 [==============================] - 202s 1ms/step - loss: 2.1071\n",
            "\n",
            "Epoch 00002: loss improved from 2.31433 to 2.10710, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-02-2.1071-bigger.hdf5\n",
            "Epoch 3/100\n",
            "180283/180283 [==============================] - 205s 1ms/step - loss: 1.9461\n",
            "\n",
            "Epoch 00003: loss improved from 2.10710 to 1.94609, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-03-1.9461-bigger.hdf5\n",
            "Epoch 4/100\n",
            "180283/180283 [==============================] - 200s 1ms/step - loss: 1.8400\n",
            "\n",
            "Epoch 00004: loss improved from 1.94609 to 1.84005, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-04-1.8400-bigger.hdf5\n",
            "Epoch 5/100\n",
            "180283/180283 [==============================] - 203s 1ms/step - loss: 1.7597\n",
            "\n",
            "Epoch 00005: loss improved from 1.84005 to 1.75966, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-05-1.7597-bigger.hdf5\n",
            "Epoch 6/100\n",
            "180283/180283 [==============================] - 199s 1ms/step - loss: 1.6945\n",
            "\n",
            "Epoch 00006: loss improved from 1.75966 to 1.69448, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-06-1.6945-bigger.hdf5\n",
            "Epoch 7/100\n",
            "180283/180283 [==============================] - 203s 1ms/step - loss: 1.6724\n",
            "\n",
            "Epoch 00007: loss improved from 1.69448 to 1.67238, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-07-1.6724-bigger.hdf5\n",
            "Epoch 8/100\n",
            "180283/180283 [==============================] - 198s 1ms/step - loss: 1.6814\n",
            "\n",
            "Epoch 00008: loss did not improve from 1.67238\n",
            "Epoch 9/100\n",
            "180283/180283 [==============================] - 201s 1ms/step - loss: 1.6222\n",
            "\n",
            "Epoch 00009: loss improved from 1.67238 to 1.62220, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-09-1.6222-bigger.hdf5\n",
            "Epoch 10/100\n",
            "180283/180283 [==============================] - 197s 1ms/step - loss: 1.5855\n",
            "\n",
            "Epoch 00010: loss improved from 1.62220 to 1.58547, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-10-1.5855-bigger.hdf5\n",
            "Epoch 11/100\n",
            "180283/180283 [==============================] - 198s 1ms/step - loss: 1.5552\n",
            "\n",
            "Epoch 00011: loss improved from 1.58547 to 1.55524, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-11-1.5552-bigger.hdf5\n",
            "Epoch 12/100\n",
            "180283/180283 [==============================] - 198s 1ms/step - loss: 1.5279\n",
            "\n",
            "Epoch 00012: loss improved from 1.55524 to 1.52794, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-12-1.5279-bigger.hdf5\n",
            "Epoch 13/100\n",
            "180283/180283 [==============================] - 191s 1ms/step - loss: 1.5014\n",
            "\n",
            "Epoch 00013: loss improved from 1.52794 to 1.50140, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-13-1.5014-bigger.hdf5\n",
            "Epoch 14/100\n",
            "180283/180283 [==============================] - 193s 1ms/step - loss: 1.4758\n",
            "\n",
            "Epoch 00014: loss improved from 1.50140 to 1.47577, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-14-1.4758-bigger.hdf5\n",
            "Epoch 15/100\n",
            "180283/180283 [==============================] - 190s 1ms/step - loss: 1.4503\n",
            "\n",
            "Epoch 00015: loss improved from 1.47577 to 1.45030, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-15-1.4503-bigger.hdf5\n",
            "Epoch 16/100\n",
            "180283/180283 [==============================] - 193s 1ms/step - loss: 1.4272\n",
            "\n",
            "Epoch 00016: loss improved from 1.45030 to 1.42716, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-16-1.4272-bigger.hdf5\n",
            "Epoch 17/100\n",
            "180283/180283 [==============================] - 188s 1ms/step - loss: 1.4020\n",
            "\n",
            "Epoch 00017: loss improved from 1.42716 to 1.40195, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-17-1.4020-bigger.hdf5\n",
            "Epoch 18/100\n",
            "180283/180283 [==============================] - 195s 1ms/step - loss: 1.3819\n",
            "\n",
            "Epoch 00018: loss improved from 1.40195 to 1.38186, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-18-1.3819-bigger.hdf5\n",
            "Epoch 19/100\n",
            "180283/180283 [==============================] - 188s 1ms/step - loss: 1.3602\n",
            "\n",
            "Epoch 00019: loss improved from 1.38186 to 1.36024, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-19-1.3602-bigger.hdf5\n",
            "Epoch 20/100\n",
            "180283/180283 [==============================] - 191s 1ms/step - loss: 1.3392\n",
            "\n",
            "Epoch 00020: loss improved from 1.36024 to 1.33918, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-20-1.3392-bigger.hdf5\n",
            "Epoch 21/100\n",
            "180283/180283 [==============================] - 188s 1ms/step - loss: 1.3217\n",
            "\n",
            "Epoch 00021: loss improved from 1.33918 to 1.32168, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-21-1.3217-bigger.hdf5\n",
            "Epoch 22/100\n",
            "180283/180283 [==============================] - 192s 1ms/step - loss: 1.3074\n",
            "\n",
            "Epoch 00022: loss improved from 1.32168 to 1.30742, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-22-1.3074-bigger.hdf5\n",
            "Epoch 23/100\n",
            "180283/180283 [==============================] - 189s 1ms/step - loss: 1.2919\n",
            "\n",
            "Epoch 00023: loss improved from 1.30742 to 1.29191, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-23-1.2919-bigger.hdf5\n",
            "Epoch 24/100\n",
            "180283/180283 [==============================] - 187s 1ms/step - loss: 1.2752\n",
            "\n",
            "Epoch 00024: loss improved from 1.29191 to 1.27518, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-24-1.2752-bigger.hdf5\n",
            "Epoch 25/100\n",
            "180283/180283 [==============================] - 190s 1ms/step - loss: 1.2630\n",
            "\n",
            "Epoch 00025: loss improved from 1.27518 to 1.26304, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-25-1.2630-bigger.hdf5\n",
            "Epoch 26/100\n",
            "180283/180283 [==============================] - 183s 1ms/step - loss: 1.2522\n",
            "\n",
            "Epoch 00026: loss improved from 1.26304 to 1.25220, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-26-1.2522-bigger.hdf5\n",
            "Epoch 27/100\n",
            "180283/180283 [==============================] - 186s 1ms/step - loss: 1.2369\n",
            "\n",
            "Epoch 00027: loss improved from 1.25220 to 1.23686, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-27-1.2369-bigger.hdf5\n",
            "Epoch 28/100\n",
            "180283/180283 [==============================] - 183s 1ms/step - loss: 1.2236\n",
            "\n",
            "Epoch 00028: loss improved from 1.23686 to 1.22361, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-28-1.2236-bigger.hdf5\n",
            "Epoch 29/100\n",
            "180283/180283 [==============================] - 189s 1ms/step - loss: 1.2140\n",
            "\n",
            "Epoch 00029: loss improved from 1.22361 to 1.21404, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-29-1.2140-bigger.hdf5\n",
            "Epoch 30/100\n",
            "180283/180283 [==============================] - 183s 1ms/step - loss: 1.2015\n",
            "\n",
            "Epoch 00030: loss improved from 1.21404 to 1.20149, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-30-1.2015-bigger.hdf5\n",
            "Epoch 31/100\n",
            "180283/180283 [==============================] - 188s 1ms/step - loss: 1.1899\n",
            "\n",
            "Epoch 00031: loss improved from 1.20149 to 1.18992, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-31-1.1899-bigger.hdf5\n",
            "Epoch 32/100\n",
            "180283/180283 [==============================] - 186s 1ms/step - loss: 1.1814\n",
            "\n",
            "Epoch 00032: loss improved from 1.18992 to 1.18142, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-32-1.1814-bigger.hdf5\n",
            "Epoch 33/100\n",
            "180283/180283 [==============================] - 189s 1ms/step - loss: 1.1705\n",
            "\n",
            "Epoch 00033: loss improved from 1.18142 to 1.17046, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-33-1.1705-bigger.hdf5\n",
            "Epoch 34/100\n",
            "180283/180283 [==============================] - 187s 1ms/step - loss: 1.1624\n",
            "\n",
            "Epoch 00034: loss improved from 1.17046 to 1.16243, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-34-1.1624-bigger.hdf5\n",
            "Epoch 35/100\n",
            "180283/180283 [==============================] - 187s 1ms/step - loss: 1.1526\n",
            "\n",
            "Epoch 00035: loss improved from 1.16243 to 1.15259, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-35-1.1526-bigger.hdf5\n",
            "Epoch 36/100\n",
            "180283/180283 [==============================] - 188s 1ms/step - loss: 1.1421\n",
            "\n",
            "Epoch 00036: loss improved from 1.15259 to 1.14211, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-36-1.1421-bigger.hdf5\n",
            "Epoch 37/100\n",
            "180283/180283 [==============================] - 183s 1ms/step - loss: 1.1320\n",
            "\n",
            "Epoch 00037: loss improved from 1.14211 to 1.13203, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-37-1.1320-bigger.hdf5\n",
            "Epoch 38/100\n",
            "180283/180283 [==============================] - 187s 1ms/step - loss: 1.1245\n",
            "\n",
            "Epoch 00038: loss improved from 1.13203 to 1.12451, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-38-1.1245-bigger.hdf5\n",
            "Epoch 39/100\n",
            "180283/180283 [==============================] - 183s 1ms/step - loss: 1.1173\n",
            "\n",
            "Epoch 00039: loss improved from 1.12451 to 1.11729, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-39-1.1173-bigger.hdf5\n",
            "Epoch 40/100\n",
            "180283/180283 [==============================] - 188s 1ms/step - loss: 1.1068\n",
            "\n",
            "Epoch 00040: loss improved from 1.11729 to 1.10680, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-40-1.1068-bigger.hdf5\n",
            "Epoch 41/100\n",
            "180283/180283 [==============================] - 186s 1ms/step - loss: 1.0976\n",
            "\n",
            "Epoch 00041: loss improved from 1.10680 to 1.09761, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-41-1.0976-bigger.hdf5\n",
            "Epoch 42/100\n",
            "180283/180283 [==============================] - 189s 1ms/step - loss: 1.0886\n",
            "\n",
            "Epoch 00042: loss improved from 1.09761 to 1.08860, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-42-1.0886-bigger.hdf5\n",
            "Epoch 43/100\n",
            "180283/180283 [==============================] - 187s 1ms/step - loss: 1.0800\n",
            "\n",
            "Epoch 00043: loss improved from 1.08860 to 1.08005, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-43-1.0800-bigger.hdf5\n",
            "Epoch 44/100\n",
            "180283/180283 [==============================] - 185s 1ms/step - loss: 1.0708\n",
            "\n",
            "Epoch 00044: loss improved from 1.08005 to 1.07084, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-44-1.0708-bigger.hdf5\n",
            "Epoch 45/100\n",
            "180283/180283 [==============================] - 188s 1ms/step - loss: 1.0660\n",
            "\n",
            "Epoch 00045: loss improved from 1.07084 to 1.06599, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-45-1.0660-bigger.hdf5\n",
            "Epoch 46/100\n",
            "180283/180283 [==============================] - 184s 1ms/step - loss: 1.0562\n",
            "\n",
            "Epoch 00046: loss improved from 1.06599 to 1.05616, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-46-1.0562-bigger.hdf5\n",
            "Epoch 47/100\n",
            "180283/180283 [==============================] - 188s 1ms/step - loss: 1.0500\n",
            "\n",
            "Epoch 00047: loss improved from 1.05616 to 1.04997, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-47-1.0500-bigger.hdf5\n",
            "Epoch 48/100\n",
            "180283/180283 [==============================] - 183s 1ms/step - loss: 1.0409\n",
            "\n",
            "Epoch 00048: loss improved from 1.04997 to 1.04090, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-48-1.0409-bigger.hdf5\n",
            "Epoch 49/100\n",
            "180283/180283 [==============================] - 186s 1ms/step - loss: 1.0339\n",
            "\n",
            "Epoch 00049: loss improved from 1.04090 to 1.03389, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-49-1.0339-bigger.hdf5\n",
            "Epoch 50/100\n",
            "180283/180283 [==============================] - 183s 1ms/step - loss: 1.0291\n",
            "\n",
            "Epoch 00050: loss improved from 1.03389 to 1.02913, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-50-1.0291-bigger.hdf5\n",
            "Epoch 51/100\n",
            "180283/180283 [==============================] - 187s 1ms/step - loss: 1.0212\n",
            "\n",
            "Epoch 00051: loss improved from 1.02913 to 1.02122, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-51-1.0212-bigger.hdf5\n",
            "Epoch 52/100\n",
            "180283/180283 [==============================] - 183s 1ms/step - loss: 1.0123\n",
            "\n",
            "Epoch 00052: loss improved from 1.02122 to 1.01235, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-52-1.0123-bigger.hdf5\n",
            "Epoch 53/100\n",
            "180283/180283 [==============================] - 186s 1ms/step - loss: 1.0065\n",
            "\n",
            "Epoch 00053: loss improved from 1.01235 to 1.00652, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-53-1.0065-bigger.hdf5\n",
            "Epoch 54/100\n",
            "180283/180283 [==============================] - 187s 1ms/step - loss: 0.9994\n",
            "\n",
            "Epoch 00054: loss improved from 1.00652 to 0.99939, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-54-0.9994-bigger.hdf5\n",
            "Epoch 55/100\n",
            "180283/180283 [==============================] - 187s 1ms/step - loss: 0.9920\n",
            "\n",
            "Epoch 00055: loss improved from 0.99939 to 0.99204, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-55-0.9920-bigger.hdf5\n",
            "Epoch 56/100\n",
            "180283/180283 [==============================] - 186s 1ms/step - loss: 0.9872\n",
            "\n",
            "Epoch 00056: loss improved from 0.99204 to 0.98716, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-56-0.9872-bigger.hdf5\n",
            "Epoch 57/100\n",
            "180283/180283 [==============================] - 187s 1ms/step - loss: 0.9789\n",
            "\n",
            "Epoch 00057: loss improved from 0.98716 to 0.97891, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-57-0.9789-bigger.hdf5\n",
            "Epoch 58/100\n",
            "180283/180283 [==============================] - 185s 1ms/step - loss: 0.9731\n",
            "\n",
            "Epoch 00058: loss improved from 0.97891 to 0.97311, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-58-0.9731-bigger.hdf5\n",
            "Epoch 59/100\n",
            "180283/180283 [==============================] - 188s 1ms/step - loss: 0.9676\n",
            "\n",
            "Epoch 00059: loss improved from 0.97311 to 0.96758, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-59-0.9676-bigger.hdf5\n",
            "Epoch 60/100\n",
            "180283/180283 [==============================] - 187s 1ms/step - loss: 0.9624\n",
            "\n",
            "Epoch 00060: loss improved from 0.96758 to 0.96238, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-60-0.9624-bigger.hdf5\n",
            "Epoch 61/100\n",
            "180283/180283 [==============================] - 188s 1ms/step - loss: 0.9531\n",
            "\n",
            "Epoch 00061: loss improved from 0.96238 to 0.95306, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-61-0.9531-bigger.hdf5\n",
            "Epoch 62/100\n",
            "180283/180283 [==============================] - 188s 1ms/step - loss: 0.9498\n",
            "\n",
            "Epoch 00062: loss improved from 0.95306 to 0.94979, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-62-0.9498-bigger.hdf5\n",
            "Epoch 63/100\n",
            "180283/180283 [==============================] - 187s 1ms/step - loss: 0.9561\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.94979\n",
            "Epoch 64/100\n",
            "180283/180283 [==============================] - 186s 1ms/step - loss: 0.9427\n",
            "\n",
            "Epoch 00064: loss improved from 0.94979 to 0.94271, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-64-0.9427-bigger.hdf5\n",
            "Epoch 65/100\n",
            "180283/180283 [==============================] - 186s 1ms/step - loss: 0.9333\n",
            "\n",
            "Epoch 00065: loss improved from 0.94271 to 0.93333, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-65-0.9333-bigger.hdf5\n",
            "Epoch 66/100\n",
            "180283/180283 [==============================] - 186s 1ms/step - loss: 0.9297\n",
            "\n",
            "Epoch 00066: loss improved from 0.93333 to 0.92967, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-66-0.9297-bigger.hdf5\n",
            "Epoch 67/100\n",
            "180283/180283 [==============================] - 186s 1ms/step - loss: 0.9289\n",
            "\n",
            "Epoch 00067: loss improved from 0.92967 to 0.92889, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-67-0.9289-bigger.hdf5\n",
            "Epoch 68/100\n",
            "180283/180283 [==============================] - 185s 1ms/step - loss: 0.9229\n",
            "\n",
            "Epoch 00068: loss improved from 0.92889 to 0.92292, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-68-0.9229-bigger.hdf5\n",
            "Epoch 69/100\n",
            "180283/180283 [==============================] - 185s 1ms/step - loss: 0.9099\n",
            "\n",
            "Epoch 00069: loss improved from 0.92292 to 0.90986, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-69-0.9099-bigger.hdf5\n",
            "Epoch 70/100\n",
            "180283/180283 [==============================] - 183s 1ms/step - loss: 0.9070\n",
            "\n",
            "Epoch 00070: loss improved from 0.90986 to 0.90696, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-70-0.9070-bigger.hdf5\n",
            "Epoch 71/100\n",
            "180283/180283 [==============================] - 188s 1ms/step - loss: 0.9037\n",
            "\n",
            "Epoch 00071: loss improved from 0.90696 to 0.90374, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-71-0.9037-bigger.hdf5\n",
            "Epoch 72/100\n",
            "180283/180283 [==============================] - 193s 1ms/step - loss: 0.8955\n",
            "\n",
            "Epoch 00072: loss improved from 0.90374 to 0.89554, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-72-0.8955-bigger.hdf5\n",
            "Epoch 73/100\n",
            "180283/180283 [==============================] - 200s 1ms/step - loss: 0.8917\n",
            "\n",
            "Epoch 00073: loss improved from 0.89554 to 0.89168, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-73-0.8917-bigger.hdf5\n",
            "Epoch 74/100\n",
            "180283/180283 [==============================] - 196s 1ms/step - loss: 0.8830\n",
            "\n",
            "Epoch 00074: loss improved from 0.89168 to 0.88301, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-74-0.8830-bigger.hdf5\n",
            "Epoch 75/100\n",
            "180283/180283 [==============================] - 200s 1ms/step - loss: 0.8807\n",
            "\n",
            "Epoch 00075: loss improved from 0.88301 to 0.88074, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-75-0.8807-bigger.hdf5\n",
            "Epoch 76/100\n",
            "180283/180283 [==============================] - 198s 1ms/step - loss: 0.8816\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.88074\n",
            "Epoch 77/100\n",
            "180283/180283 [==============================] - 201s 1ms/step - loss: 0.8736\n",
            "\n",
            "Epoch 00077: loss improved from 0.88074 to 0.87360, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-77-0.8736-bigger.hdf5\n",
            "Epoch 78/100\n",
            "180283/180283 [==============================] - 200s 1ms/step - loss: 0.8636\n",
            "\n",
            "Epoch 00078: loss improved from 0.87360 to 0.86362, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-78-0.8636-bigger.hdf5\n",
            "Epoch 79/100\n",
            "180283/180283 [==============================] - 197s 1ms/step - loss: 0.8631\n",
            "\n",
            "Epoch 00079: loss improved from 0.86362 to 0.86309, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-79-0.8631-bigger.hdf5\n",
            "Epoch 80/100\n",
            "180283/180283 [==============================] - 202s 1ms/step - loss: 0.8587\n",
            "\n",
            "Epoch 00080: loss improved from 0.86309 to 0.85871, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-80-0.8587-bigger.hdf5\n",
            "Epoch 81/100\n",
            "180283/180283 [==============================] - 197s 1ms/step - loss: 0.8536\n",
            "\n",
            "Epoch 00081: loss improved from 0.85871 to 0.85358, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-81-0.8536-bigger.hdf5\n",
            "Epoch 82/100\n",
            "180283/180283 [==============================] - 201s 1ms/step - loss: 0.8480\n",
            "\n",
            "Epoch 00082: loss improved from 0.85358 to 0.84801, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-82-0.8480-bigger.hdf5\n",
            "Epoch 83/100\n",
            "180283/180283 [==============================] - 196s 1ms/step - loss: 0.8453\n",
            "\n",
            "Epoch 00083: loss improved from 0.84801 to 0.84534, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-83-0.8453-bigger.hdf5\n",
            "Epoch 84/100\n",
            "180283/180283 [==============================] - 198s 1ms/step - loss: 0.8394\n",
            "\n",
            "Epoch 00084: loss improved from 0.84534 to 0.83942, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-84-0.8394-bigger.hdf5\n",
            "Epoch 85/100\n",
            "180283/180283 [==============================] - 197s 1ms/step - loss: 0.8346\n",
            "\n",
            "Epoch 00085: loss improved from 0.83942 to 0.83462, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-85-0.8346-bigger.hdf5\n",
            "Epoch 86/100\n",
            "180283/180283 [==============================] - 203s 1ms/step - loss: 0.8299\n",
            "\n",
            "Epoch 00086: loss improved from 0.83462 to 0.82988, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-86-0.8299-bigger.hdf5\n",
            "Epoch 87/100\n",
            "180283/180283 [==============================] - 199s 1ms/step - loss: 0.8261\n",
            "\n",
            "Epoch 00087: loss improved from 0.82988 to 0.82611, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-87-0.8261-bigger.hdf5\n",
            "Epoch 88/100\n",
            "180283/180283 [==============================] - 203s 1ms/step - loss: 0.8239\n",
            "\n",
            "Epoch 00088: loss improved from 0.82611 to 0.82390, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-88-0.8239-bigger.hdf5\n",
            "Epoch 89/100\n",
            "180283/180283 [==============================] - 202s 1ms/step - loss: 0.8248\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.82390\n",
            "Epoch 90/100\n",
            "180283/180283 [==============================] - 197s 1ms/step - loss: 0.8149\n",
            "\n",
            "Epoch 00090: loss improved from 0.82390 to 0.81491, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-90-0.8149-bigger.hdf5\n",
            "Epoch 91/100\n",
            "180283/180283 [==============================] - 201s 1ms/step - loss: 0.8102\n",
            "\n",
            "Epoch 00091: loss improved from 0.81491 to 0.81021, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-91-0.8102-bigger.hdf5\n",
            "Epoch 92/100\n",
            "180283/180283 [==============================] - 199s 1ms/step - loss: 0.8062\n",
            "\n",
            "Epoch 00092: loss improved from 0.81021 to 0.80622, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-92-0.8062-bigger.hdf5\n",
            "Epoch 93/100\n",
            "180283/180283 [==============================] - 201s 1ms/step - loss: 0.8028\n",
            "\n",
            "Epoch 00093: loss improved from 0.80622 to 0.80281, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-93-0.8028-bigger.hdf5\n",
            "Epoch 94/100\n",
            "180283/180283 [==============================] - 197s 1ms/step - loss: 0.8003\n",
            "\n",
            "Epoch 00094: loss improved from 0.80281 to 0.80034, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-94-0.8003-bigger.hdf5\n",
            "Epoch 95/100\n",
            "180283/180283 [==============================] - 202s 1ms/step - loss: 0.7956\n",
            "\n",
            "Epoch 00095: loss improved from 0.80034 to 0.79557, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-95-0.7956-bigger.hdf5\n",
            "Epoch 96/100\n",
            "180283/180283 [==============================] - 202s 1ms/step - loss: 0.7906\n",
            "\n",
            "Epoch 00096: loss improved from 0.79557 to 0.79062, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-96-0.7906-bigger.hdf5\n",
            "Epoch 97/100\n",
            "180283/180283 [==============================] - 202s 1ms/step - loss: 0.7897\n",
            "\n",
            "Epoch 00097: loss improved from 0.79062 to 0.78969, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-97-0.7897-bigger.hdf5\n",
            "Epoch 98/100\n",
            "180283/180283 [==============================] - 203s 1ms/step - loss: 0.7824\n",
            "\n",
            "Epoch 00098: loss improved from 0.78969 to 0.78236, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-98-0.7824-bigger.hdf5\n",
            "Epoch 99/100\n",
            "180283/180283 [==============================] - 202s 1ms/step - loss: 0.7813\n",
            "\n",
            "Epoch 00099: loss improved from 0.78236 to 0.78128, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-99-0.7813-bigger.hdf5\n",
            "Epoch 100/100\n",
            "180283/180283 [==============================] - 201s 1ms/step - loss: 0.7746\n",
            "\n",
            "Epoch 00100: loss improved from 0.78128 to 0.77463, saving model to drive/LSTM_TextGen/Models/Model5/weights-improvement-100-0.7746-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7d2dddbe10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1IHuPh9Ze9A",
        "colab_type": "text"
      },
      "source": [
        "#### Testing the model\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKp0dknoVEBY",
        "colab_type": "code",
        "outputId": "0820c3fa-19ca-4fd8-f1eb-6ddaff31ac94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# pick a random seed\n",
        "result = ''\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = list(dataX[start])\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult += int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\t#print(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\"  moment           lets go on with the game the queen said to alice and alice was    too much frighte \"\n",
            "\n",
            "Done.\n",
            "ned toletced inm tuattely and ie to ofat the hidged                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3CMqI7sKyr-",
        "colab_type": "text"
      },
      "source": [
        "### Generate Text using above created model\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ4fF-85Zpnx",
        "colab_type": "text"
      },
      "source": [
        "#### Load saved model to generate text\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCSWkgCjMbfs",
        "colab_type": "code",
        "outputId": "2b1e2ab6-b114-4255-b163-93db1c0197a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "import string\n",
        "import math\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n",
        "orig_text = open(default_path + filename).read()\n",
        "orig_text = orig_text.lower()\n",
        "\n",
        "# remove punctuations\n",
        "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "raw_text = regex.sub('', orig_text)\n",
        "\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))[1:]\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "\n",
        "lines = [line for line in raw_text.splitlines() if line.isspace()==False and len(line)>0]\n",
        "#print(lines)\n",
        "\n",
        "sent_len_list = [len(line) for line in lines]\n",
        "max_sent_len = max(sent_len_list)\n",
        "min_sent_len = min(sent_len_list)\n",
        "avg_sent_len = math.floor(sum(sent_len_list)/len(sent_len_list))\n",
        "common_sent_len = max(set(sent_len_list), key=sent_len_list.count)\n",
        "print(f\"Maximum length of sentence - {max_sent_len}\")\n",
        "print(f\"Minimum length of sentence - {min_sent_len}\")\n",
        "print(f\"Average length of sentences - {avg_sent_len}\")\n",
        "print(f\"Common length of sentences - {common_sent_len}\")\n",
        "\n",
        "inp_sequences = [list(line) for line in lines]\n",
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "data = []\n",
        "for line in lines:\n",
        "\tdata.append([char_to_int[char] for char in line])\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# pad sequence\n",
        "data_padded = pad_sequences(data, maxlen=max_sent_len)\n",
        "flat_list = [item for sublist in data_padded for item in sublist]\n",
        "\n",
        "input_len = len(flat_list)\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, input_len - seq_length, 1):\n",
        "    # Define input and output sequences\n",
        "    # Input is the current character plus desired sequence length\n",
        "    in_seq = flat_list[i:i + seq_length]\n",
        "\n",
        "    # Out sequence is the initial character plus total sequence length\n",
        "    out_seq = flat_list[i + seq_length]\n",
        "\n",
        "    # We now convert list of characters to integers based on\n",
        "    # previously and add the values to our lists\n",
        "    dataX.append([char for char in in_seq])\n",
        "    dataY.append(out_seq)\n",
        "\n",
        "n_patterns = len(dataX)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "# load the network weights\n",
        "filename = \"drive/LSTM_TextGen/\" + \"Models/Model5/\" + \"weights-improvement-100-0.7746-bigger.hdf5\" #\"weights-improvement-47-1.2219-bigger.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  136087\n",
            "Total Vocab:  29\n",
            "Maximum length of sentence - 73\n",
            "Minimum length of sentence - 2\n",
            "Average length of sentences - 53\n",
            "Common length of sentences - 67\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0727 16:11:19.572988 140031781689216 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0727 16:11:19.609420 140031781689216 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0727 16:11:19.615979 140031781689216 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0727 16:11:19.809154 140031781689216 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0727 16:11:19.820449 140031781689216 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0727 16:11:23.126463 140031781689216 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0727 16:11:26.169478 140031781689216 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWofd1SqZ1IK",
        "colab_type": "text"
      },
      "source": [
        "#### Generate text from created model\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ei_DjJIrh0v",
        "colab_type": "code",
        "outputId": "9dac4fd5-b0ee-48be-a514-5acebe0644ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "gen_sents = []\n",
        "for sents in range(10):\n",
        "  # pick a random seed\n",
        "  result = ''\n",
        "  start = np.random.randint(0, len(dataX)-1)\n",
        "  pattern = list(dataX[start])\n",
        "  print(\"Seed:\")\n",
        "  print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "  # generate characters\n",
        "  for i in range(500):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result += int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "  gen_sents.append(result)\n",
        "  #print(result)\n",
        "  \n",
        "print(\"Generated Text:\")\n",
        "print(\"\\n\".join(gen_sents))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\"            a cat may look at a king said alice ive read that in some book                            \"\n",
            "Seed:\n",
            "\" ith you mind      now the poor little thing sobbed again or grunted it was impossible                \"\n",
            "Seed:\n",
            "\"  a pleasure in all their simple joys                  remembering her own childlife and the happy su \"\n",
            "Seed:\n",
            "\" t them free                                                       exactly as we were                 \"\n",
            "Seed:\n",
            "\" ake out    at all what had become of it so after hunting all about for it he was    obliged to write \"\n",
            "Seed:\n",
            "\" er if i shall ever see you        any more and here poor alice began to cry again for she felt very  \"\n",
            "Seed:\n",
            "\" he fan and the pair of white kid gloves    and she very goodnaturedly began hunting about for them b \"\n",
            "Seed:\n",
            "\"  the accident of   the goldfish kept running in her head and she had a vague sort of idea     that t \"\n",
            "Seed:\n",
            "\"              as far out to sea as you can                                     swim after them scream \"\n",
            "Seed:\n",
            "\" ice       come lets try the first figure said the mock turtle to the gryphon                     we  \"\n",
            "Generated Text:\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            "ite hind                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            " the felt a little turpe said tooe toder and when sut                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            "nice ceganed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
            "ume beot ieard intoed to the wholg alice help of tuch alice whosg                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
            " anice repeame                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
            "wand tathar inm secd alice as helg wo hit so sfad                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6wHgLwUaG3f",
        "colab_type": "text"
      },
      "source": [
        "**Generated Text**:\n",
        "\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
        "ite hind                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
        " the felt a little turpe said tooe toder and when sut                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
        "nice ceganed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
        "ume beot ieard intoed to the wholg alice help of tuch alice whosg                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
        " anice repeame                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
        "wand tathar inm secd alice as helg wo hit so sfad"
      ]
    }
  ]
}